<!DOCTYPE html>
<!-- saved from url=(0031)https://linjiwang.com/post/gan/ -->
<html lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="IE=edge"><meta name="generator" content="Wowchemy 5.7.0 for Hugo"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preload" as="style" href="./data/css2"><link rel="stylesheet" href="./data/css2" media="all" onload="this.media=&quot;all&quot;"><link rel="stylesheet" href="./data/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="all" onload="this.media=&quot;all&quot;"><link rel="stylesheet" href="./data/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="all" onload="this.media=&quot;all&quot;"><link rel="stylesheet" href="./data/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css"><link rel="stylesheet" href="./data/github-light.min.css" title="hl-light" media="all" onload="this.media=&quot;all&quot;" disabled=""><link rel="stylesheet" href="./data/dracula.min.css" title="hl-dark" media="all" onload="this.media=&quot;all&quot;"><meta name="author" content="Linji Wang (王琳箕)"><meta name="description" content="In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."><link rel="alternate" hreflang="en-us" href="https://linjiwang.com/post/gan/"><link rel="canonical" href="https://linjiwang.com/post/gan/"><link rel="manifest" href="https://linjiwang.com/manifest.webmanifest"><link rel="icon" type="image/png" href="https://linjiwang.com/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_32x32_fill_lanczos_center_3.png"><link rel="apple-touch-icon" type="image/png" href="https://linjiwang.com/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_180x180_fill_lanczos_center_3.png"><meta name="theme-color" content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://linjiwang.com/post/gan/featured.png"><meta property="og:site_name" content="Academic"><meta property="og:url" content="https://linjiwang.com/post/gan/"><meta property="og:title" content="When Cats meet GANs | Academic"><meta property="og:description" content="In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."><meta property="og:image" content="https://linjiwang.com/post/gan/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-20T20:14:30-04:00"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://linjiwang.com/post/gan/"},"headline":"When Cats meet GANs","image":["https://linjiwang.com/post/gan/featured.png"],"datePublished":"2019-02-05T00:00:00Z","dateModified":"2023-03-20T20:14:30-04:00","author":{"@type":"Person","name":"Linji Wang (王琳箕)"},"publisher":{"@type":"Organization","name":"Academic","logo":{"@type":"ImageObject","url":"https://linjiwang.com/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_192x192_fill_lanczos_center_3.png"}},"description":"In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."}</script><title>When Cats meet GANs | Academic</title><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head><body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper dark" data-wc-page-id="cf2bf6beb32d4133b075c3f3e2208511" data-new-gr-c-s-check-loaded="14.1027.0" data-gr-ext-installed=""><script src="./data/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js.download"></script><aside class="search-modal" id="search"><div class="container"><section class="search-header"><div class="row no-gutters justify-content-between mb-3"><div class="col-6"><h1>Search</h1></div><div class="col-6 col-search-close"><a class="js-search" href="https://linjiwang.com/post/gan/#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a></div></div><div id="search-box"><input name="q" id="search-query" placeholder="Search..." autocapitalize="off" autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control" aria-label="Search..."></div></section><section class="section-search-results"><div id="search-hits"></div></section></div></aside><div class="page-header header--fixed headroom headroom--not-bottom headroom--pinned headroom--top"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main"><div class="container-xl"><div class="d-none d-lg-inline-flex"><a class="navbar-brand" href="https://linjiwang.com/">Academic</a></div><button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class="navbar-brand" href="https://linjiwang.com/">Academic</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"><ul class="navbar-nav d-md-inline-flex"><li class="nav-item"><a class="nav-link" href="https://linjiwang.com/#about"><span>Home</span></a></li><li class="nav-item"><a class="nav-link" href="https://linjiwang.com/#posts"><span>Posts</span></a></li><li class="nav-item"><a class="nav-link" href="https://linjiwang.com/#projects"><span>Projects</span></a></li><li class="nav-item"><a class="nav-link" href="https://linjiwang.com/#contact"><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item"><a class="nav-link js-search" href="https://linjiwang.com/post/gan/#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a></li><li class="nav-item dropdown theme-dropdown"><a href="https://linjiwang.com/post/gan/#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences"><i class="fas fa-moon" aria-hidden="true"></i></a><div class="dropdown-menu"><a href="https://linjiwang.com/post/gan/#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href="https://linjiwang.com/post/gan/#" class="dropdown-item js-set-theme-dark dropdown-item-active"><span>Dark</span></a>
<a href="https://linjiwang.com/post/gan/#" class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class="page-body"><article class="article"><div class="article-container pt-3"><h1>When Cats meet GANs</h1><p class="page-subtitle">A Comprehensive Study on DCGANs and CycleGANs with Advanced Augmentation Techniques</p><div class="article-metadata"><div><span class="author-highlighted">Linji Wang (王琳箕)</span></div><span class="article-date">Last updated on
Mar 20, 2023</span>
<span class="middot-divider"></span>
<span class="article-reading-time">11 min read</span>
<span class="middot-divider"></span>
<span class="article-categories"><i class="fas fa-folder mr-1"></i><a href="https://linjiwang.com/category/project/">Project</a></span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width:512px;max-height:256px"><div style="position:relative"><img src="./data/featured_hu4288bdcdb3699d80052a4f5c670366f0_287815_1200x2500_fit_q75_h2_lanczos_3.webp" width="512" height="256" alt="" class="featured-image">
<span class="article-header-caption">DCGAN Results</span></div></div><div class="article-container"><div class="article-style"><!-- raw HTML omitted --><details class="toc-inpage d-print-none" open=""><summary class="font-weight-bold">Table of Contents</summary><nav id="TableOfContents" class="nav flex-column"><ul><li class="nav-item"><a href="https://linjiwang.com/post/gan/#introduction" class="nav-link">Introduction</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#part-1-deep-convolutional-gan" class="nav-link">Part 1: Deep Convolutional GAN</a><ul><li class="nav-item"><a href="https://linjiwang.com/post/gan/#implement-data-augmentation" class="nav-link">Implement Data Augmentation</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#implement-the-discriminator-of-the-dcgan" class="nav-link">Implement the Discriminator of the DCGAN</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#generator" class="nav-link">Generator</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#training-loop" class="nav-link">Training Loop</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#experiment-with-dcgans" class="nav-link">Experiment with DCGANs</a></li></ul></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#part-2-cyclegan" class="nav-link">Part 2: CycleGAN</a><ul><li class="nav-item"><a href="https://linjiwang.com/post/gan/#data-augmentation" class="nav-link">Data Augmentation</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#generator-1" class="nav-link">Generator</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#training-loop-1" class="nav-link">Training Loop</a></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#experiment-with-cyclegan" class="nav-link">Experiment with CycleGAN</a></li></ul></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#bells--whistles" class="nav-link">Bells &amp; Whistles</a><ul><li class="nav-item"><a href="https://linjiwang.com/post/gan/#implement-and-train-a-diffusion-model" class="nav-link">Implement and train a diffusion model</a></li></ul></li><li class="nav-item"><a href="https://linjiwang.com/post/gan/#conclusion-1" class="nav-link">Conclusion</a></li></ul></nav></details><h2 id="introduction">Introduction</h2><p>In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:</p><p>Implementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise.
Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.</p><h2 id="part-1-deep-convolutional-gan">Part 1: Deep Convolutional GAN</h2><p>For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).</p><h3 id="implement-data-augmentation">Implement Data Augmentation</h3><p>Implemented the deluxe version of data augmentation in ‘data_loader.py’.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#66d9ef">elif</span> opts<span style="color:#f92672">.</span>data_preprocess <span style="color:#f92672">==</span> <span style="color:#e6db74">'deluxe'</span>:
</span></span><span style="display:flex"><span>        load_size <span style="color:#f92672">=</span> int(<span style="color:#ae81ff">1.1</span> <span style="color:#f92672">*</span> opts<span style="color:#f92672">.</span>image_size)
</span></span><span style="display:flex"><span>        osize <span style="color:#f92672">=</span> [load_size, load_size]
</span></span><span style="display:flex"><span>        deluxe_transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex"><span>        transforms<span style="color:#f92672">.</span>Resize(opts<span style="color:#f92672">.</span>image_size, Image<span style="color:#f92672">.</span>BICUBIC),
</span></span><span style="display:flex"><span>        transforms<span style="color:#f92672">.</span>RandomCrop(opts<span style="color:#f92672">.</span>image_size),
</span></span><span style="display:flex"><span>        transforms<span style="color:#f92672">.</span>RandomHorizontalFlip(),
</span></span><span style="display:flex"><span>        transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex"><span>        transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>), (<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>)),
</span></span><span style="display:flex"><span>        ])
</span></span><span style="display:flex"><span>        train_transform <span style="color:#f92672">=</span> deluxe_transform
</span></span><span style="display:flex"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h3 id="implement-the-discriminator-of-the-dcgan">Implement the Discriminator of the DCGAN</h3><p>(Answer for padding calculation goes here)</p><p>Implemented the architecture by filling in the ‘<strong>init</strong>’ and ‘forward’ method of the ‘DCDiscriminator’ class in ‘models.py’.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#66d9ef">def</span> __init__(self, conv_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>):
</span></span><span style="display:flex"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv4 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv5 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex"><span>    <span style="color:#e6db74">"""Forward pass, x is (B, C, H, W)."""</span>
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv2(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv3(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv4(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv5(x)
</span></span><span style="display:flex"><span>    <span style="color:#66d9ef">return</span> x<span style="color:#f92672">.</span>squeeze()
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h3 id="generator">Generator</h3><p>Implemented the generator of the DCGAN by filling in the ‘<strong>init</strong>’ and ‘forward’ method of the ‘DCGenerator’ class in ‘models.py’.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#66d9ef">def</span> __init__(self, noise_size, conv_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>):
</span></span><span style="display:flex"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv1 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#e6db74">'instance'</span>, <span style="color:#66d9ef">False</span>,<span style="color:#e6db74">'relu'</span> )
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv2 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv3 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv4 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv5 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'tanh'</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, z):
</span></span><span style="display:flex"><span>    <span style="color:#e6db74">"""
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Generate an image given a sample of random noise.
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Input
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    -----
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">        z: BS x noise_size x 1 x 1   --&gt;  16x100x1x1
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Output
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    ------
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">        out: BS x channels x image_width x image_height  --&gt;  16x3x64x64
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    """</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv1(z)
</span></span><span style="display:flex"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv2(z)
</span></span><span style="display:flex"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv3(z)
</span></span><span style="display:flex"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv4(z)
</span></span><span style="display:flex"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv5(z)
</span></span><span style="display:flex"><span>    <span style="color:#66d9ef">return</span> z
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h3 id="training-loop">Training Loop</h3><p>Implemented the training loop for the DCGAN by filling in the indicated parts of the training_loop function in vanilla_gan.py.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span>            <span style="color:#75715e"># TRAIN THE DISCRIMINATOR</span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 1. Compute the discriminator loss on real images</span>
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">if</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>                D_real_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(DiffAugment(real_images, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>                D_real_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(real_images) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 2. Sample noise</span>
</span></span><span style="display:flex"><span>            noise <span style="color:#f92672">=</span> sample_noise(opts<span style="color:#f92672">.</span>batch_size, opts<span style="color:#f92672">.</span>noise_size)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 3. Generate fake images from the noise</span>
</span></span><span style="display:flex"><span>            fake_images <span style="color:#f92672">=</span> G(noise)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 4. Compute the discriminator loss on the fake images</span>
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">if</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>                D_fake_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(DiffAugment(fake_images<span style="color:#f92672">.</span>detach(), policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> ))) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>                D_real_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(fake_images<span style="color:#f92672">.</span>detach())) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>            D_total_loss <span style="color:#f92672">=</span> (D_real_loss <span style="color:#f92672">+</span> D_fake_loss) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># update the discriminator D</span>
</span></span><span style="display:flex"><span>            d_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex"><span>            D_total_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex"><span>            d_optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># TRAIN THE GENERATOR</span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 1. Sample noise</span>
</span></span><span style="display:flex"><span>            noise <span style="color:#f92672">=</span> sample_noise(opts<span style="color:#f92672">.</span>batch_size, opts<span style="color:#f92672">.</span>noise_size)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 2. Generate fake images from the noise</span>
</span></span><span style="display:flex"><span>            fake_images <span style="color:#f92672">=</span> G(noise)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>            <span style="color:#75715e"># 3. Compute the generator loss</span>
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">if</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>                G_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(DiffAugment(fake_images, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> ))<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>                G_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D(fake_images)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h4 id="differentiable-augmentation">Differentiable Augmentation</h4><p>(Discussion of results with and without applying differentiable augmentations, and the difference between two augmentation schemes in terms of implementation and effects)</p><h3 id="experiment-with-dcgans">Experiment with DCGANs</h3><p>We’ve been experimenting with different data preprocessing techniques, and we’ve found that the choice of preprocessing can have a significant impact on the performance of the GAN. To demonstrate this, we’ve included screenshots of the training loss for both the discriminator and generator with two different preprocessing options: basic, deluxe and diff_aug.</p><h4 id="grumpifybprocessed_basic">grumpifyBprocessed_basic</h4><p></p><figure id="figure-sample-data_preprocessbasic-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-006400.png" alt="sample: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>sample: data_preprocess=basic, iter = 6400</figcaption></figure><figure id="figure-d_fake_loss-data_preprocessbasic-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss.png" alt="D_fake_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_fake_loss: data_preprocess=basic, iter = 6400</figcaption></figure><figure id="figure-d_real_loss-data_preprocessbasic-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss.png" alt="D_real_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_real_loss: data_preprocess=basic, iter = 6400</figcaption></figure><figure id="figure-d_total_loss-data_preprocessbasic-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_total_loss.png" alt="D_total_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_total_loss: data_preprocess=basic, iter = 6400</figcaption></figure><figure id="figure-g_loss-data_preprocessbasic-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss.png" alt="G_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>G_loss: data_preprocess=basic, iter = 6400</figcaption></figure><p></p><h4 id="grumpifybprocessed_deluxe">grumpifyBprocessed_deluxe</h4><p></p><figure id="figure-data_preprocessdeluxe-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-006400(1).png" alt="data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>data_preprocess=deluxe, iter = 6400</figcaption></figure><figure id="figure-d_fake_loss-data_preprocessdeluxe-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(1).png" alt="D_fake_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_fake_loss: data_preprocess=deluxe, iter = 6400</figcaption></figure><figure id="figure-d_real_loss-data_preprocessdeluxe-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(1).png" alt="D_real_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_real_loss: data_preprocess=deluxe, iter = 6400</figcaption></figure><figure id="figure-d_total_loss-data_preprocessdeluxe-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_total_loss(1).png" alt="D_total_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_total_loss: data_preprocess=deluxe, iter = 6400</figcaption></figure><figure id="figure-g_loss-data_preprocessdeluxe-iter--6400"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(1).png" alt="G_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>G_loss: data_preprocess=deluxe, iter = 6400</figcaption></figure><figure id="figure-data_preprocessdeluxe-iter--6400-diff_aug--true"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-006400(2).png" alt="data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>data_preprocess=deluxe, iter = 6400, diff_aug = True</figcaption></figure><p></p><h4 id="grumpifybprocessed_deluxe_diffaug">grumpifyBprocessed_deluxe_diffaug</h4><p></p><figure id="figure-d_fake_loss-data_preprocessdeluxe-iter--6400-diff_aug--true"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(2).png" alt="D_fake_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_fake_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True</figcaption></figure><figure id="figure-d_real_loss-data_preprocessdeluxe-iter--6400-diff_aug--true"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(2).png" alt="D_real_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_real_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True</figcaption></figure><figure id="figure-d_total_loss-data_preprocessdeluxe-iter--6400-diff_aug--true"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_total_loss(2).png" alt="D_total_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>D_total_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True</figcaption></figure><figure id="figure-g_loss-data_preprocessdeluxe-iter--6400-diff_aug--true"><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(2).png" alt="G_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div><figcaption>G_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True</figcaption></figure><p></p><h4 id="results-analysis">Results analysis</h4><table class=".table"><thead><tr><th>Data Preprocessing</th><th>Discriminator Loss</th><th>Generator Loss</th><th>Convergence Rate</th><th>Stability</th></tr></thead><tbody><tr><td>Basic</td><td>Slow decrease, potential instability</td><td>Fluctuates, struggles to generate realistic images</td><td>Slow</td><td>Less stable</td></tr><tr><td>Deluxe</td><td>Faster decrease, more effective at differentiation</td><td>Converges more quickly, learns from more varied examples</td><td>Faster</td><td>More stable</td></tr><tr><td>Differential Augmentations</td><td>Even faster decrease, more effective at differentiation</td><td>Faster generation of diverse and realistic images</td><td>Fastest</td><td>Most stable</td></tr></tbody></table><p>The table above highlights the key differences in the loss curves for a DCGAN trained with different data preprocessing techniques. Basic preprocessing techniques result in slower convergence rates and potentially less stable loss curves, while deluxe techniques result in faster convergence and more stable loss curves. The most effective approach is to use differential augmentations, where different augmentation policies are applied to real and fake images, resulting in the fastest convergence and the most stable loss curves. This analysis suggests that the choice of data preprocessing techniques can have a significant impact on the performance of a GAN, and careful consideration should be given to selecting the most effective approach.</p><h2 id="part-2-cyclegan">Part 2: CycleGAN</h2><p>Implemented the CycleGAN architecture.</p><h3 id="data-augmentation">Data Augmentation</h3><p>Set the –data_preprocess flag to deluxe.</p><h3 id="generator-1">Generator</h3><p>Implemented the generator architecture by completing the <strong>init</strong> method of the CycleGenerator class in models.py.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#66d9ef">def</span> __init__(self, conv_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, init_zero_weights<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>):
</span></span><span style="display:flex"><span>    super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># # 1. Define the encoder part of the generator</span>
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> conv(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, norm, <span style="color:#66d9ef">False</span>, <span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># # 2. Define the transformation part of the generator</span>
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>resnet_block <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(ResnetBlock(conv_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, norm <span style="color:#f92672">=</span> norm, activ <span style="color:#f92672">=</span> <span style="color:#e6db74">'relu'</span>),
</span></span><span style="display:flex"><span>                                      ResnetBlock(conv_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, norm <span style="color:#f92672">=</span> norm, activ <span style="color:#f92672">=</span> <span style="color:#e6db74">'relu'</span>),
</span></span><span style="display:flex"><span>                                      ResnetBlock(conv_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>, norm <span style="color:#f92672">=</span> norm, activ <span style="color:#f92672">=</span> <span style="color:#e6db74">'relu'</span>),)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># # 3. Define the decoder part of the generator</span>
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv1 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span><span style="color:#e6db74">'instance'</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'relu'</span>)
</span></span><span style="display:flex"><span>    self<span style="color:#f92672">.</span>up_conv2 <span style="color:#f92672">=</span> up_conv(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, norm<span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>, activ<span style="color:#f92672">=</span><span style="color:#e6db74">'tanh'</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex"><span>    <span style="color:#e6db74">"""
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Generate an image conditioned on an input image.
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Input
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    -----
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">        x: BS x 3 x 32 x 32
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    Output
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    ------
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">        out: BS x 3 x 32 x 32
</span></span></span><span style="display:flex"><span><span style="color:#e6db74">    """</span>
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv2(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>resnet_block(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv1(x)
</span></span><span style="display:flex"><span>    x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up_conv2(x)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#66d9ef">return</span> x
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h3 id="training-loop-1">Training Loop</h3><p>Implemented the training loop for the CycleGAN by filling in the indicated parts of the training_loop function in cycle_gan.py.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#75715e"># TRAIN THE DISCRIMINATORS</span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 1. Compute the discriminator losses on real images</span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>    D_X_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(images_X) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>    D_Y_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_Y(images_Y) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>    D_X_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(DiffAugment(images_X, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>    D_Y_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_Y(DiffAugment(images_Y, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>d_real_loss <span style="color:#f92672">=</span> D_X_loss <span style="color:#f92672">+</span> D_Y_loss
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 2. Generate domain-X-like images based on real images in domain Y</span>
</span></span><span style="display:flex"><span>fake_X <span style="color:#f92672">=</span> G_YtoX(images_Y)<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 3. Compute the loss for D_X</span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    D_X_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(fake_X) ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>    D_X_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(DiffAugment(fake_X, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 4. Generate domain-Y-like images based on real images in domain X</span>
</span></span><span style="display:flex"><span>fake_Y <span style="color:#f92672">=</span> G_XtoY(images_X)<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 5. Compute the loss for D_Y</span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    D_Y_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_Y(fake_Y) ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>    D_Y_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_Y(DiffAugment(fake_Y, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>d_fake_loss <span style="color:#f92672">=</span> D_X_loss <span style="color:#f92672">+</span> D_Y_loss
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># sum up the losses and update D_X and D_Y</span>
</span></span><span style="display:flex"><span>d_optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex"><span>d_total_loss <span style="color:#f92672">=</span> d_real_loss <span style="color:#f92672">+</span> d_fake_loss
</span></span><span style="display:flex"><span>d_total_loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex"><span>d_optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># plot the losses in tensorboard</span>
</span></span><span style="display:flex"><span>logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'D/XY/real'</span>, D_X_loss, iteration)
</span></span><span style="display:flex"><span>logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'D/YX/real'</span>, D_Y_loss, iteration)
</span></span><span style="display:flex"><span>logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'D/XY/fake'</span>, D_X_loss, iteration)
</span></span><span style="display:flex"><span>logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'D/YX/fake'</span>, D_Y_loss, iteration)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># TRAIN THE GENERATORS</span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 1. Generate domain-X-like images based on real images in domain Y</span>
</span></span><span style="display:flex"><span>fake_X <span style="color:#f92672">=</span> G_YtoX(images_Y)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 2. Compute the generator loss based on domain X</span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    g_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(fake_X) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>    g_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((D_X(DiffAugment(fake_X, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'G/XY/fake'</span>, g_loss, iteration)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> opts<span style="color:#f92672">.</span>use_cycle_consistency_loss:
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># 3. Compute the cycle consistency loss (the reconstruction loss)</span>
</span></span><span style="display:flex"><span>    cycle_consistency_loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((images_X <span style="color:#f92672">-</span> G_YtoX(G_XtoY(images_X)))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    g_loss <span style="color:#f92672">+=</span> opts<span style="color:#f92672">.</span>lambda_cycle <span style="color:#f92672">*</span> cycle_consistency_loss
</span></span><span style="display:flex"><span>    logger<span style="color:#f92672">.</span>add_scalar(<span style="color:#e6db74">'G/XY/cycle'</span>, opts<span style="color:#f92672">.</span>lambda_cycle <span style="color:#f92672">*</span> cycle_consistency_loss, iteration)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># X--Y--&gt;X CYCLE</span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 1. Generate domain-Y-like images based on real images in domain X</span>
</span></span><span style="display:flex"><span>fake_Y <span style="color:#f92672">=</span> G_XtoY(images_X)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span><span style="color:#75715e"># 2. Compute the generator loss based on domain Y</span>
</span></span><span style="display:flex"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> opts<span style="color:#f92672">.</span>use_diffaug:
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    g_loss <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>mean((D_Y(fake_Y) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex"><span>    g_loss <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>mean((D_Y(DiffAugment(fake_Y, policy<span style="color:#f92672">=</span><span style="color:#e6db74">'color,translation,cutout'</span>, channels_first<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span> )) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> ) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h3 id="experiment-with-cyclegan">Experiment with CycleGAN</h3><p>INSERT IMAGE: Two example images of generated Grumpy cats from Russian Blue cats, and two example images of generated Russian Blue cats from Grumpy cats.</p><!-- raw HTML omitted --><h4 id="cat_10deluxe_instance_dc_cycle_naive">cat_10deluxe_instance_dc_cycle_naive</h4><table class=".table"><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Image</th></tr></thead><tbody><tr><td style="text-align:center">sample X to Y</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-X-Y.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">sample Y to X</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-Y-X.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_fake_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(3).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_real_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(3).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_X_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_X_loss.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_Y_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_Y_loss.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">G_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(3).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr></tbody></table><!-- raw HTML omitted --><h4 id="cat_10deluxe_instance_patch_cycle_naive">cat_10deluxe_instance_patch_cycle_naive</h4><table class=".table"><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Image</th></tr></thead><tbody><tr><td style="text-align:center">sample X to Y</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-X-Y(1).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">sample Y to X</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-Y-X(1).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_fake_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(4).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_real_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(4).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_X_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_X_loss(1).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_Y_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_Y_loss(1).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">G_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(4).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr></tbody></table><!-- raw HTML omitted --><h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle">cat_10deluxe_instance_patch_cycle_naive_cycle</h4><table class=".table"><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Image</th></tr></thead><tbody><tr><td style="text-align:center">sample X to Y</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-X-Y(2).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">sample Y to X</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-001000-Y-X(2).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_fake_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(5).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_real_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(5).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_X_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_X_loss(2).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_Y_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_Y_loss(2).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">G_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(5).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr></tbody></table><!-- raw HTML omitted --><h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug">cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug</h4><table class=".table"><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Image</th></tr></thead><tbody><tr><td style="text-align:center">sample X to Y</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-010000-X-Y.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">sample Y to X</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/sample-010000-Y-X.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_fake_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_fake_loss(6).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_real_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_real_loss(6).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_X_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_X_loss(3).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">D_Y_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/D_Y_loss(3).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">G_loss</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/G_loss(6).png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr></tbody></table><!-- raw HTML omitted --><h4 id="observations">Observations:</h4><p>We observed that the results with the cycle-consistency loss were better than the results without it. The translations between the two domains were more accurate and realistic. This is because the cycle-consistency loss enforces the consistency between the two translations, which helps the model to learn better.</p><p>We also observed that the DCDiscriminator resulted in better quality translations than the PatchDiscriminator. This is because the DCDiscriminator has a larger receptive field, which enables it to capture more global features of the image.</p><h4 id="conclusion">Conclusion:</h4><p>In conclusion, we have trained CycleGAN from scratch with and without the cycle-consistency loss, and have compared the results using the DCDiscriminator and the PatchDiscriminator. We have observed that the cycle-consistency loss and the DCDiscriminator resulted in better quality translations between the two domains. These observations can help in improving the translation quality between different domains in image processing applications.</p><h2 id="bells--whistles">Bells &amp; Whistles</h2><h3 id="implement-and-train-a-diffusion-model">Implement and train a diffusion model</h3><h4 id="training-diffusion-models-with-hugging-faces-diffusers">Training Diffusion Models with Hugging Face’s Diffusers</h4><h4 id="introduction-1">Introduction</h4><p>In this project, we train a simple diffusion model using the Hugging Face’s Diffusers library. Diffusion models have become state-of-the-art generative models in recent times.</p><h4 id="key-parts-of-the-code">Key Parts of the Code</h4><h5 id="configuration">Configuration:</h5><p>We define a ‘TrainingConfig’ class that holds all the training hyperparameters.
Hyperparameters include ‘image_size’, ’train_batch_size’, ’eval_batch_size’, ’num_epochs’, ‘gradient_accumulation_steps’, ’learning_rate’, and ’lr_warmup_steps’, among others.</p><h5 id="data-preprocessing">Data Preprocessing:</h5><p>We use the datasets library to load our dataset and apply data transformations.
The dataset is preprocessed using the transforms.Compose function from torchvision.
The dataset is then transformed on-the-fly during training.</p><h5 id="model-definition">Model Definition:</h5><p>We define our model using the ‘UNet2DModel’ class from the diffusers library.
The model has various hyperparameters such as ‘sample_size’, ‘in_channels’, ‘out_channels’, ’layers_per_block’, ‘block_out_channels’, ‘down_block_types’, and ‘up_block_types’.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> UNet2DModel
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>model <span style="color:#f92672">=</span> UNet2DModel(
</span></span><span style="display:flex"><span>    sample_size<span style="color:#f92672">=</span>config<span style="color:#f92672">.</span>image_size,  <span style="color:#75715e"># the target image resolution</span>
</span></span><span style="display:flex"><span>    in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,  <span style="color:#75715e"># the number of input channels, 3 for RGB images</span>
</span></span><span style="display:flex"><span>    out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,  <span style="color:#75715e"># the number of output channels</span>
</span></span><span style="display:flex"><span>    layers_per_block<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,  <span style="color:#75715e"># how many ResNet layers to use per UNet block</span>
</span></span><span style="display:flex"><span>    block_out_channels<span style="color:#f92672">=</span>(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>),  <span style="color:#75715e"># the number of output channes for each UNet block</span>
</span></span><span style="display:flex"><span>    down_block_types<span style="color:#f92672">=</span>( 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"DownBlock2D"</span>,  <span style="color:#75715e"># a regular ResNet downsampling block</span>
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"DownBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"DownBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"DownBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"AttnDownBlock2D"</span>,  <span style="color:#75715e"># a ResNet downsampling block with spatial self-attention</span>
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"DownBlock2D"</span>,
</span></span><span style="display:flex"><span>    ), 
</span></span><span style="display:flex"><span>    up_block_types<span style="color:#f92672">=</span>(
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"UpBlock2D"</span>,  <span style="color:#75715e"># a regular ResNet upsampling block</span>
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"AttnUpBlock2D"</span>,  <span style="color:#75715e"># a ResNet upsampling block with spatial self-attention</span>
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"UpBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"UpBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"UpBlock2D"</span>, 
</span></span><span style="display:flex"><span>        <span style="color:#e6db74">"UpBlock2D"</span>  
</span></span><span style="display:flex"><span>      ),
</span></span><span style="display:flex"><span>)
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h5 id="noise-scheduler">Noise Scheduler:</h5><p>We use the ‘DDPMScheduler’ class from the diffusers library to define the noise scheduler for our model.
The scheduler takes a batch of images, a batch of random noise, and the timesteps for each image.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> DDPMScheduler
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>noise_scheduler <span style="color:#f92672">=</span> DDPMScheduler(num_train_timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h4 id="training-setup">Training Setup:</h4><p>We use an AdamW optimizer and a cosine learning rate schedule for training.
We use the DDPMPipeline class from the diffusers library for end-to-end inference during evaluation.
The training function train_loop is defined, which includes gradient accumulation, mixed precision training, and multi-GPU or TPU training using the Accelerator class from the accelerate library.</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="display:flex"><span><span style="color:#66d9ef">for</span> step, batch <span style="color:#f92672">in</span> enumerate(train_dataloader):
</span></span><span style="display:flex"><span>    clean_images <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">'images'</span>]
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># Sample noise to add to the images</span>
</span></span><span style="display:flex"><span>    noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(clean_images<span style="color:#f92672">.</span>shape)<span style="color:#f92672">.</span>to(clean_images<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex"><span>    bs <span style="color:#f92672">=</span> clean_images<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># Sample a random timestep for each image</span>
</span></span><span style="display:flex"><span>    timesteps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, noise_scheduler<span style="color:#f92672">.</span>num_train_timesteps, (bs,), device<span style="color:#f92672">=</span>clean_images<span style="color:#f92672">.</span>device)<span style="color:#f92672">.</span>long()
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># Add noise to the clean images according to the noise magnitude at each timestep</span>
</span></span><span style="display:flex"><span>    <span style="color:#75715e"># (this is the forward diffusion process)</span>
</span></span><span style="display:flex"><span>    noisy_images <span style="color:#f92672">=</span> noise_scheduler<span style="color:#f92672">.</span>add_noise(clean_images, noise, timesteps)
</span></span><span style="display:flex"><span>    
</span></span><span style="display:flex"><span>    <span style="color:#66d9ef">with</span> accelerator<span style="color:#f92672">.</span>accumulate(model):
</span></span><span style="display:flex"><span>        <span style="color:#75715e"># Predict the noise residual</span>
</span></span><span style="display:flex"><span>        noise_pred <span style="color:#f92672">=</span> model(noisy_images, timesteps, return_dict<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(noise_pred, noise)
</span></span><span style="display:flex"><span>        accelerator<span style="color:#f92672">.</span>backward(loss)
</span></span><span style="display:flex"><span>
</span></span><span style="display:flex"><span>        accelerator<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex"><span>        lr_scheduler<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex"><span>            
</span></span></code></pre><button class="btn btn-primary btn-copy-code">Copy</button></div><h4 id="training-execution">Training Execution:</h4><p>We use the ’notebook_launcher’ function from the accelerate library to launch the training from the notebook.</p><h4 id="key-functions">Key Functions</h4><p><em>transform(examples)</em>: Applies the image transformations on the fly during training.
<em>evaluate(config, epoch, pipeline)</em>: Generates a batch of sample images during evaluation and saves them as a grid to the disk.
<em>train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)</em>: The main training loop, which includes the forward diffusion process, loss calculation, and backpropagation.</p><h4 id="diffusion-results">Diffusion Results</h4><table class=".table"><thead><tr><th style="text-align:center">Title</th><th style="text-align:center">Image</th></tr></thead><tbody><tr><td style="text-align:center">Apple</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/apple.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr><tr><td style="text-align:center">Cat</td><td style="text-align:center"><figure><div class="d-flex justify-content-center"><div class="w-100"><img src="./data/cat.png" alt="" loading="lazy" data-zoomable="" class="medium-zoom-image"></div></div></figure></td></tr></tbody></table><p>The quality of the generated images and how well the DCGAN has captured the main differences between the two domains depend on factors such as the quality of the training data, hyperparameters used during training, and complexity of image domains. If the diffusion results look unrealistic compared to the DCGAN results, it could be due to factors such as dataset quality, model complexity, hyperparameter tuning, or training time. Further analysis and experimentation would be necessary to pinpoint the specific reason for the difference in image quality.</p><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><h2 id="conclusion-1">Conclusion</h2><p>This report presents our implementation of DCGAN and CycleGAN for various image generation tasks. Through these experiments, we have observed the impact of data augmentation and differentiable augmentation on the training process and final results. We have also seen the capabilities of CycleGAN in generating realistic images for domain-to-domain translation tasks, such as converting Grumpy cats to Russian Blue cats and vice versa, and converting apples to oranges and vice versa.</p></div><div class="article-tags"><a class="badge badge-light" href="https://linjiwang.com/tag/computer-vision/">Computer Vision</a>
<a class="badge badge-light" href="https://linjiwang.com/tag/image-generation/">Image Generation</a>
<a class="badge badge-light" href="https://linjiwang.com/tag/deep-learning/">Deep Learning</a></div><div class="share-box"><ul class="share"><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F&amp;text=When+Cats+meet+GANs" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter"><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F&amp;t=When+Cats+meet+GANs" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook"><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=When%20Cats%20meet%20GANs&amp;body=https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope"><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F&amp;title=When+Cats+meet+GANs" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in"><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=When+Cats+meet+GANs%20https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp"><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Flinjiwang.com%2Fpost%2Fgan%2F&amp;title=When+Cats+meet+GANs" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo"><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href="https://linjiwang.com/"><img class="avatar mr-3 avatar-circle" src="./data/avatar_huc61a9cf250f63b4f157dbff5a6cdbb31_21867_270x270_fill_q75_lanczos_center.jpg" alt="Linji Wang (王琳箕)"></a><div class="media-body"><h5 class="card-title"><a href="https://linjiwang.com/">Linji Wang (王琳箕)</a></h5><h6 class="card-subtitle">Incoming PhD Student</h6><p class="card-text">My research interests include computer vision, machine learning and reinforcement learning.</p><ul class="network-icon" aria-hidden="true"><li><a href="mailto:joewwang@outlook.com"><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/in/linjiw/" target="_blank" rel="noopener"><i class="fab fa-linkedin"></i></a></li><li><a href="https://github.com/linjiw" target="_blank" rel="noopener"><i class="fab fa-github"></i></a></li><li><a href="https://linjiwang.com/files/cv.pdf"><i class="ai ai-cv"></i></a></li></ul></div></div></div></article></div><div class="page-footer"><div class="container"><footer class="site-footer"><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <!-- raw HTML omitted -->CC BY NC ND 4.0<!-- raw HTML omitted --></p><p class="powered-by footer-license-icons"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i></a></p><p class="powered-by">Published with <!-- raw HTML omitted -->Wowchemy<!-- raw HTML omitted --> — the free, <!-- raw HTML omitted -->open source<!-- raw HTML omitted --> website builder that empowers creators.</p></footer></div></div><script src="./data/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js.download"></script>
<script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src="./data/fuse.min.js.download" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
<script src="./data/jquery.mark.min.js.download" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
<script id="page-data" type="application/json">{"use_headroom":true}</script><script src="./data/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js.download" type="module"></script>
<script src="./data/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js.download"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>